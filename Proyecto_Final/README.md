# Detector de Placas Vehiculares ‚Äì Procesamiento Digital de Im√°genes

**Autores:** Felipe Id√°rraga Quintero y  Julian Felipe Guti√©rrez Ram√≠rez

**Nombre de la Pr√°ctica:** Proyecto Final

**Curso:** Desarrollo de Sistemas IoT

**Departamento:** Departamento de Ingenier√≠a Electrica, Electronica y Computacion

Este proyecto implementa un sistema completo de detecci√≥n de placas vehiculares utilizando t√©cnicas modernas de Deep Learning. Se desarrolla un modelo entrenado desde cero con un conjunto de datos de Roboflow, se eval√∫a su desempe√±o, se convierte a TorchScript para despliegue en dispositivos embebidos (como Raspberry Pi) y finalmente se integra con una API en un HuggingFace Space.

---

```text
## Estructura del repositorio

üìÇ Proyecto_Final/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ deteccion-placas-v1.ipynb                 # Notebook principal: dataset, modelo, entrenamiento, m√©tricas y exportaci√≥n
‚îÇ
‚îú‚îÄ‚îÄ üìÇ Hugging_Face/
‚îÇ   ‚îú‚îÄ‚îÄ  HFS_Proyecto_Final.ipynb              # Notebook de despliegue en HuggingFace Space (Colab)
‚îÇ   ‚îî‚îÄ‚îÄ  Inferencia_HuggingFace.py             # Cliente Python: consume /predict y dibuja detecciones
‚îÇ
‚îú‚îÄ‚îÄ üìÇ Pesos_del_Modelo/
‚îÇ   ‚îú‚îÄ‚îÄ  best_plate_detector.pth               # Pesos del modelo en PyTorch (state_dict)
‚îÇ   ‚îî‚îÄ‚îÄ  plate_detector_ts_cpu.pt              # Modelo exportado a TorchScript (CPU) para despliegue
‚îÇ
‚îî‚îÄ‚îÄ üìÇ raspberry/
    ‚îî‚îÄ‚îÄ  inferencia_raspberry.py               # Inferencia en Raspberry Pi (TorchScript + OpenCV + NMS)
```

## 1. Descripci√≥n general del proyecto

El objetivo del proyecto es desarrollar un sistema de detecci√≥n autom√°tica de placas vehiculares, partiendo desde el entrenamiento del modelo hasta su despliegue en entornos reales.

Las fases principales incluidas en este repositorio son:

**1. Carga y preparaci√≥n del dataset** desde Roboflow.

**2. Dise√±o del modelo de detecci√≥n** basado en PyTorch.

**3. Implementaci√≥n de la funci√≥n de p√©rdida personalizada.**

**4. Entrenamiento y validaci√≥n del modelo detector.**

**5. Evaluaci√≥n mediante m√©tricas de desempe√±o.**

**6. Exportaci√≥n a formato TorchScript** para despliegue eficiente

**7. Comparaci√≥n entre modelo PyTorch original (.pth) y TorchScript (.pt)**

**8. Implementaci√≥n de un script de inferencia en Raspberry Pi.**

**9. Despliegue del modelo en un HuggingFace Space mediante API.**

## 2. Dataset

El dataset fue obtenido desde **Roboflow** llamado ****, incluyendo im√°genes anotadas de placas vehiculares en distintos entornos, con las siguientes caracter√≠sticas:

- Formato YOLO
- Divisi√≥n train / valid / test
- Augmentaciones aplicadas:
  - Rotaci√≥n
  - Brillo / contraste
  - Scale jitter
  - Flip horizontal

El dataset se carga directamente en el notebook mediante la API de Roboflow:

```python
from roboflow import Roboflow
rf = Roboflow(api_key="YOUR_API_KEY")
project = rf.workspace("workspace").project("placas")
dataset = project.version(1).download("yolov8")
```

## 3. Modelo de Detecci√≥n

Se implement√≥ un modelo de detecci√≥n propio en PyTorch basado en una versi√≥n ligera de **YOLOv3 Mini mejorado**, dise√±ado espec√≠ficamente para la detecci√≥n de placas vehiculares. El modelo integra los siguientes componentes:

- **Backbone convolucional con bloques residuales**, encargado de extraer caracter√≠sticas robustas a partir de la imagen.
- **M√≥dulo SPP (Spatial Pyramid Pooling)**, que aporta contexto multi-escala utilizando max-pooling con distintos tama√±os de ventana.
- **Cabeza de detecci√≥n con anchors optimizados**, donde cada celda de la grilla predice offsets de cajas, objectness y probabilidades de clase, empleando los anchors generados por k-means.
- **Funci√≥n `decode` estilo YOLO**, responsable de transformar las predicciones de la red en coordenadas reales (p√≠xeles), scores y clases antes del post-procesamiento.
- **Funci√≥n de p√©rdida combinada** (Obj + BBox + Class) que entrena simult√°neamente la localizaci√≥n, confianza y clasificaci√≥n.

El notebook incluye:

- La **arquitectura completa del modelo** (backbone, SPP y head de detecci√≥n).
- El **proceso de entrenamiento**.
- La **validaci√≥n y m√©tricas de desempe√±o**.
- El **guardado de pesos** del modelo en formato `.pth` para uso posterior en inferencia y exportaci√≥n a TorchScript.

## 4. Funci√≥n de p√©rdida para detecci√≥n (`yolo_plate_loss`)

Se implement√≥ `yolo_plate_loss`, una p√©rdida inspirada en **YOLO** y adaptada al modelo `ImprovedPlateDetector`. A partir de las cajas reales normalizadas, la funci√≥n:

- Asigna cada placa a una **celda del grid y al mejor anchor** seg√∫n IoU.
- Construye los **targets** para posici√≥n `(tx, ty)`, tama√±o `(tw, th)`, confianza (`t_obj`) y clase (`t_cls`).
- Calcula:
  - **P√©rdida de coordenadas** (posici√≥n y tama√±o de la caja).
  - **P√©rdida de confianza** para celdas con objeto y sin objeto.
  - **P√©rdida de clasificaci√≥n** sobre las celdas que contienen placas.

La p√©rdida total combina estos t√©rminos con factores de ponderaci√≥n (`lambda_coord`, `lambda_obj`, `lambda_noobj`) y devuelve:

- Un **escalar `total_loss`** usado para el entrenamiento.
- Un **diccionario `loss_dict`** con el desglose: `coord_loss`, `conf_loss` y `cls_loss`.

## 5. M√©tricas de desempe√±o

Durante el proceso de evaluaci√≥n del modelo se calcularon:

- **P√©rdida promedio** en validaci√≥n (loss total y desglosada por componentes).
- **AP@0.5 (AP50)** como m√©trica principal de detecci√≥n de placas.
- **Precisi√≥n, recall y F1 por clase**, junto con conteo de TP, FP y FN.

Adem√°s, se generan visualizaciones de:

- **Evoluci√≥n de la p√©rdida** (entrenamiento vs. validaci√≥n) a lo largo de las √©pocas.
- **Evoluci√≥n del AP@0.5** (entrenamiento vs. validaci√≥n) en porcentaje.

## 6. Conversi√≥n a TorchScript

Para facilitar el despliegue del detector en entornos fuera de PyTorch ‚Äúcompleto‚Äù (por ejemplo, inferencia en **CPU** y dispositivos embebidos), el modelo se export√≥ a **TorchScript**. En el notebook se realiza la conversi√≥n reconstruyendo el `ImprovedPlateDetector` con **los mismos par√°metros y anchors del entrenamiento**, cargando los pesos `.pth` y generando una versi√≥n TorchScript mediante **trazado** (`torch.jit.trace`) con una entrada dummy.

Se exportan dos variantes:

- **`plate_detector_ts.pt`**: trazado usando el dispositivo disponible (GPU si est√° disponible).
- **`plate_detector_ts_cpu.pt`**: trazado y guardado espec√≠ficamente para **CPU**, recomendado para despliegue (p. ej., Raspberry Pi).

Ejemplo (como se hace en el notebook):

```python
# Reconstruir el modelo con los mismos par√°metros del entrenamiento
loaded_model = ImprovedPlateDetector(
    num_classes=1,
    image_size=(416, 416),
    num_anchors=3,
    anchors=anchors_kmeans.tolist()
).to(device)

# Cargar pesos y exportar a TorchScript por trace
loaded_model.load_state_dict(torch.load("best_plate_detector.pth", map_location=device))
loaded_model.eval()

dummy_input = torch.randn(1, 3, 416, 416, device=device)

with torch.no_grad():
    ts_model = torch.jit.trace(loaded_model, dummy_input)

ts_model.save("plate_detector_ts_cpu.pt")
print(" Modelo exportado a TorchScript (CPU)")
```

## 7. Comparaci√≥n entre PyTorch (.pth) y TorchScript (.pt)

Para analizar el impacto de la exportaci√≥n a TorchScript, se compararon ambas versiones del detector:

- **Modelo PyTorch (eager):** `best_plate_detector.pth`
- **Modelo TorchScript (CPU):** `plate_detector_ts_cpu.pt`

### Aspectos evaluados

1. **Tama√±o en disco (MB)**  
   Se compar√≥ el tama√±o del archivo `.pth` frente al `.pt` para verificar si exist√≠a alguna reducci√≥n tras la conversi√≥n.

2. **Velocidad de inferencia (benchmark)**  
   Se midi√≥ el tiempo promedio de inferencia (forward) de ambos modelos usando la misma entrada dummy, incluyendo *warmup* y m√∫ltiples ejecuciones para estimar media y desviaci√≥n est√°ndar.

3. **Consistencia num√©rica (opcional / si aplica en el notebook)**  
   Cuando se realiza, se comparan las salidas de ambos modelos sobre las mismas entradas calculando:
   - `max |Œî|`: diferencia absoluta m√°xima
   - `mean |Œî|`: diferencia absoluta promedio

###  Resultados observados (ejemplo)

- **Tama√±o:** no se evidenci√≥ reducci√≥n significativa (los archivos quedaron con tama√±os similares).
- **Tiempo de inferencia:** el rendimiento fue comparable entre ambas versiones (speedup cercano a `1.0x`).

```text
 Tama√±o best_plate_detector.pth      : 38.08 MB
 Tama√±o plate_detector_ts_cpu.pt     : 38.27 MB

PyTorch (eager):      299.799 ¬± 4.276 ms
TorchScript (.pt):    303.697 ¬± 8.123 ms
Speedup aproximado:      0.99x
```
## 8. Inferencia en Raspberry Pi

El repositorio incluye un script de inferencia para ejecutar el detector de placas en **CPU** (ideal para Raspberry Pi) usando el modelo exportado a **TorchScript** (`.pt`). El script carga el modelo, procesa una imagen desde ruta y genera una salida con las detecciones dibujadas.

### Archivo incluido

`raspberry/inference_plate_detector_ts.py` *(ajusta el nombre/ruta si en tu repo es diferente)*

### Funcionalidades del script

- **Carga** un modelo TorchScript (`torch.jit.load`) desde una ruta (por defecto `plate_detector_ts_cpu.pt`).
- **Preprocesa** la imagen con OpenCV:
  - BGR ‚Üí RGB
  - redimensiona a `416√ó416`
  - normaliza a `[0,1]`
  - convierte a tensor `[1,3,H,W]`
- **Realiza inferencia** en CPU.
- **Decodifica** la salida tipo YOLO (grid + anchors) para obtener cajas `(x1,y1,x2,y2)`, score y clase.
- **Filtra por confianza** y aplica **Non-Max Suppression (NMS)**.
- **Reescala** las cajas al tama√±o original de la imagen.
- **Dibuja** las detecciones y **guarda** el resultado como `output_detection.jpg`.

### Ejecuci√≥n

```bash
python3 inference_plate_detector_ts.py plate_detector_ts_cpu.pt ruta/imagen.jpg 0.25 0.5
```

**Par√°metros:**
- `0.25`: umbral de confianza (*conf_threshold*).
- `0.5`: umbral IoU para NMS (*nms_iou_threshold*).

### Requisitos en Raspberry Pi

Instalar dependencias b√°sicas:

```bash
sudo apt-get update
sudo apt-get install -y python3-opencv
pip3 install numpy
```

**Nota:** para ejecutar un modelo TorchScript .pt tambi√©n necesitas PyTorch instalado en la Raspberry Pi (porque torch.jit.load depende de torch). La instalaci√≥n en ARM var√≠a seg√∫n el modelo y el sistema operativo, pero TorchScript permite desplegar el modelo sin depender del c√≥digo fuente original y facilita la ejecuci√≥n en CPU.

## 9. Despliegue en HuggingFace Space

El proyecto incluye un notebook de despliegue en Google Colab para publicar el detector en **HuggingFace Spaces** mediante una **API en FastAPI**.

üìÑ `HFS_Proyecto_Final.ipynb` *

En el notebook se realiza el siguiente flujo:

1. **Implementaci√≥n de la API (FastAPI)**  
   Se construye una aplicaci√≥n que:
   - Carga el modelo de detecci√≥n en formato **TorchScript** (`torch.jit.load`).
   - Define los esquemas de entrada/salida con Pydantic.
   - Incluye utilidades de **preprocesamiento** (imagen ‚Üí tensor 416√ó416) y **decodificaci√≥n** de predicciones (YOLO + anchors + NMS).
   - Expone endpoints:
     - `/` (ruta base)
     - `/health` (verificaci√≥n de estado)
     - `/predict` (inferencia)

2. **Endpoint `/predict`**  
   Recibe una imagen codificada en **Base64** y retorna un JSON con las detecciones:

   - Coordenadas de caja: `(x1, y1, x2, y2)` *(en el sistema de 416√ó416)*
   - Puntaje de confianza: `score`
   - Identificador y nombre de clase: `class_id`, `class_name` (ej. `"LicensePlate"`)

3. **Dockerfile para el Space**  
   Se define un `Dockerfile` para el despliegue que:
   - Usa **Python 3.10**
   - Instala dependencias desde `requirements.txt`
   - Copia el c√≥digo de la app al contenedor
   - Expone el puerto `7860`
   - Inicia el servidor con **Uvicorn** (`app:app`)

4. **Cliente Python para consumir la API**  
   Se incluye un script de ejemplo que:
   - Lee una imagen desde disco
   - La codifica en Base64
   - Realiza una petici√≥n POST al endpoint `/predict`
   - Dibuja y opcionalmente guarda las detecciones sobre la imagen

### Ejemplo de consumo desde un cliente Python

```python
import base64
import requests

url = "https://<tu-space>.hf.space/predict"

with open("placa_test.jpg", "rb") as f:
    img_b64 = base64.b64encode(f.read()).decode("utf-8")

payload = {"image_base64": img_b64}
resp = requests.post(url, json=payload)

print(resp.json())
```

## 10. Flujo de uso recomendado

Para reproducir el proyecto de principio a fin, se recomienda seguir el siguiente flujo de trabajo:

### 1Ô∏è‚É£ Abrir el notebook principal

`deteccion-placas-v1.ipynb`

### 2Ô∏è‚É£ Ejecutar las secciones en orden

1. **Carga del dataset desde Roboflow.**
2. **Definici√≥n del modelo** y su **funci√≥n de p√©rdida personalizada**.
3. **Entrenamiento** del modelo.
4. **Validaci√≥n** y c√°lculo de las **m√©tricas de desempe√±o**.
5. **Guardado de pesos** en formato `.pth`.
6. **Conversi√≥n del modelo a TorchScript** (`.pt`).
7. **Comparaci√≥n entre las salidas de PyTorch y TorchScript**.

---

### 3Ô∏è‚É£ Probar el despliegue embebido (Raspberry Pi)

1. Copiar a la Raspberry Pi:
   - `plate_detector_ts_cpu.pt`
   - `inferencia_raspberry.py`
2. Ejecutar pruebas utilizando im√°genes reales para validar el modelo.

---

### 4Ô∏è‚É£ Probar el despliegue en la nube (HuggingFace Space)

1. Abrir el notebook de despliegue en HuggingFace.
2. Subir el modelo TorchScript.
3. Verificar el funcionamiento de la API usando solicitudes de inferencia.
4. Comprobar que retorna detecciones correctas (coordenadas, score, clase).

---

Este flujo garantiza que el proyecto pueda reproducirse completamente desde el entrenamiento hasta el despliegue final, tanto en dispositivos locales como en la nube.

## 11. Requisitos

### 11.1. Entorno de entrenamiento (Kaggle / PC)

Requisitos m√≠nimos:

- **Python 3.9+**
- Bibliotecas principales:
  - `torch`
  - `torchvision`
  - `numpy`
  - `matplotlib`
  - `seaborn`
  - `tqdm`
  - `roboflow`
  - `opencv-python`
  - `fastapi` (para pruebas de API locales)
  - `uvicorn` (servidor ASGI)

#### Instalaci√≥n t√≠pica

```bash
pip install torch torchvision roboflow opencv-python matplotlib seaborn tqdm fastapi uvicorn
```

### 11.2. Entorno en Raspberry Pi

**Requisitos m√≠nimos:**

- **Python 3**

- **Bibliotecas necesarias:**
  - `numpy`
  - `opencv-python`

- **Compatibilidad con PyTorch / TorchScript** para arquitectura ARM.  
  *(Se puede instalar PyTorch para ARM o usar √∫nicamente el runtime de TorchScript).*

#### Instalaci√≥n de dependencias

```bash
pip3 install numpy opencv-python
```
